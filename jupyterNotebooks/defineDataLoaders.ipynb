{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorboard.backend.event_processing import event_accumulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadTensorboardTensorData(enty_tag, folder_path):\n",
    "    \"\"\"\n",
    "    Loads the tensor data of the log entry with the given entry tag from the given path.\n",
    "    :param enty_tag: (String) The tag of the entry from which to get the data.\n",
    "    :param folder_path: (String) The path to the log dir.\n",
    "    :return: data_per_step: (Tupel) (step_nums, data) The data for each step.\n",
    "    \"\"\"\n",
    "    ea = event_accumulator.EventAccumulator(folder_path, size_guidance={'tensors': 0})\n",
    "    ea.Reload()\n",
    "    try:\n",
    "        _, step_nums, data = zip(*ea.Tensors(enty_tag))\n",
    "    except:\n",
    "        _, step_nums, data = zip(*ea.Tensors(\"SingleGPUTraining/\"+enty_tag))  \n",
    "    return (step_nums[:-1], data[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadModelTensorboardData(dir_prefix, dir_suffix, model_xFolds, loss_enty_tag =\"loss\", obj_loss_enty_tag = \"obj_loss\", reg_loss_enty_tag = \"reg_loss\", acc_enty_tag = None):\n",
    "    \"\"\"\n",
    "    Loads the tensoboard data of the model given by dir_prefix, dir_suffix and its xFolds.\n",
    "    \n",
    "    :param dir_prefix: (String) The prefix of the dir from which to get the data.\n",
    "    :param dir_suffix: (String) The suffix of the dir from which to get the data.\n",
    "    :param model_xFolds: (Array of Integer) The xFolds of the models.\n",
    "    :param loss_enty_tag: (String): The tag of the overall losses in the tensorboard data.\n",
    "    :param loss_enty_tag: (String): The tag of the objective losses in the tensorboard data.\n",
    "    :param loss_enty_tag: (String): The tag of the regularization losses in the tensorboard data.\n",
    "    :param loss_enty_tag: (String): The tag of the accuracies in the tensorboard data.\n",
    "    \"\"\"\n",
    "    steps_per_xFold=[[] for _ in range(len(model_xFolds))]\n",
    "    losses_per_xFold=[[] for _ in range(len(model_xFolds))]\n",
    "    obj_losses_per_xFold=[[] for _ in range(len(model_xFolds))]\n",
    "    reg_losses_per_xFold=[[] for _ in range(len(model_xFolds))]\n",
    "    accs_per_xFold=[[] for _ in range(len(model_xFolds))]\n",
    "    \n",
    "    first_round = True\n",
    "    \n",
    "    for xFold in model_xFolds:\n",
    "        steps, losses = loadTensorboardTensorData(loss_enty_tag, dir_prefix + str(xFold) + dir_suffix)\n",
    "        sort_index= np.argsort(np.array(steps))            \n",
    "        steps, obj_losses = loadTensorboardTensorData(obj_loss_enty_tag, dir_prefix + str(xFold) + dir_suffix)\n",
    "        steps, reg_losses = loadTensorboardTensorData(reg_loss_enty_tag, dir_prefix + str(xFold) + dir_suffix)\n",
    "      \n",
    "        if acc_enty_tag:\n",
    "            steps, accs = loadTensorboardTensorData(acc_enty_tag, dir_prefix + str(xFold) + dir_suffix)\n",
    "            accs = np.array(accs)[sort_index]\n",
    "        else:\n",
    "            accs = np.zeros(len(steps))\n",
    "          \n",
    "        steps = np.array(steps)[sort_index]\n",
    "        losses = np.array(losses)[sort_index]\n",
    "        obj_losses = np.array(obj_losses)[sort_index]\n",
    "        reg_losses = np.array(reg_losses)[sort_index]\n",
    "\n",
    "        if not first_round:\n",
    "            if not (steps_per_xFold[xFold-1] == steps).all():\n",
    "                print(\"Error xFolds lens do not match! \")\n",
    "                return None\n",
    "        \n",
    "        steps_per_xFold[xFold] = steps\n",
    "        \n",
    "        for loss, obj_loss, reg_loss, acc in zip(losses, obj_losses, reg_losses, accs):\n",
    "            losses_per_xFold[xFold].append(tf.io.decode_raw(loss.tensor_content, loss.dtype).numpy()[0])\n",
    "            obj_losses_per_xFold[xFold].append(tf.io.decode_raw(obj_loss.tensor_content, obj_loss.dtype).numpy()[0])\n",
    "            reg_losses_per_xFold[xFold].append(tf.io.decode_raw(reg_loss.tensor_content, reg_loss.dtype).numpy()[0])\n",
    "            if acc_enty_tag:\n",
    "                accs_per_xFold[xFold].append(tf.io.decode_raw(acc.tensor_content, acc.dtype).numpy()[0])    \n",
    "        first_round = False     \n",
    "   \n",
    "    return steps_per_xFold, losses_per_xFold, obj_losses_per_xFold, reg_losses_per_xFold, accs_per_xFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadPretextModelsEvalData(data_path, dataset_name, pretext_model_names, xFolds, steps_per_epoch, loss_enty_tag =\"loss\", obj_loss_enty_tag = \"obj_loss\", reg_loss_enty_tag = \"reg_loss\", acc_enty_tags = None):\n",
    "    \"\"\"\n",
    "    Loads the evaluation data of the pretext models given by data_path, dataset_name, pretext_model_names and xFolds.\n",
    "    \n",
    "    :param data_path: (String) The path from which to get the data.\n",
    "    :param dataset_name: (String) The names of the datasaset for which to get the data.\n",
    "    :param pretext_model_names: (Array of Strings) The name of the pretext models for which to get the data.\n",
    "    :param xFolds: (Array of Integer) The xFold of the pretext models.\n",
    "    :param steps_per_epoch: (Integer) The number of steps in an epoch.\n",
    "    :param loss_enty_tag: (String): The tag of the overall losses in the tensorboard data.\n",
    "    :param loss_enty_tag: (String): The tag of the objective losses in the tensorboard data.\n",
    "    :param loss_enty_tag: (String): The tag of the regularization losses in the tensorboard data.\n",
    "    :param loss_enty_tag: (String): The tag of the accuracies in the tensorboard data.\n",
    "    \"\"\"\n",
    "    epochs_per_xFold_per_model=[]\n",
    "    losses_per_xFold_per_model=[]\n",
    "    obj_losses_per_xFold_per_model=[]\n",
    "    reg_losses_per_xFold_per_model=[]\n",
    "    accs_per_xFold_per_model=[]\n",
    "    \n",
    "    acc_enty_tag = None\n",
    "    for pretext_model_name in pretext_model_names:\n",
    "        dir_prefix = data_path + \"/\"+ pretext_model_name + \"/\" + dataset_name + \"/xFoldCrossVal_\"\n",
    "        dir_suffix = \"/logs/eval\"\n",
    "        \n",
    "        if acc_enty_tags:\n",
    "            acc_enty_tag = acc_enty_tags[pretext_model_name]\n",
    "            \n",
    "        steps_per_xFold, losses_per_xFold, obj_losses_per_xFold, reg_losses_per_xFold, accs_per_xFold = loadModelTensorboardData(dir_prefix, dir_suffix, xFolds, loss_enty_tag =loss_enty_tag, obj_loss_enty_tag = obj_loss_enty_tag, reg_loss_enty_tag = reg_loss_enty_tag, acc_enty_tag = acc_enty_tag)\n",
    "        \n",
    "        steps_per_xFold = np.array(steps_per_xFold)\n",
    "        epochs_per_xFold_per_model.append(np.divide(steps_per_xFold, steps_per_epoch))\n",
    "        losses_per_xFold_per_model.append(losses_per_xFold)\n",
    "        obj_losses_per_xFold_per_model.append(obj_losses_per_xFold)\n",
    "        reg_losses_per_xFold_per_model.append(reg_losses_per_xFold)\n",
    "        accs_per_xFold_per_model.append(accs_per_xFold)\n",
    "        \n",
    "    return epochs_per_xFold_per_model, losses_per_xFold_per_model, obj_losses_per_xFold_per_model, reg_losses_per_xFold_per_model, accs_per_xFold_per_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dont ask me why I used text files\n",
    "def loadTargetModelEvalTxtFile(path, target_model_name, dataset_name, pretext_model_name, pretext_model_xFold):\n",
    "    \"\"\"\n",
    "    Loads the stats of the target models trained on every specified pretext training step for the given dataset. \n",
    "    \n",
    "    :param path: (String) The path from which to get the states.\n",
    "    :param target_model_name: (String) The name of the target model for which to get the data every trained checkpoint.\n",
    "    :param dataset_name: (String) The names of the datasaset for which to get the data for every trained checkpoint.\n",
    "    :param pretext_model_name: (String) The name of the pretext model the target model was trained on.\n",
    "    :param pretext_model_xFold: (Integer) The xFold of the pretext model the target model was trained on.\n",
    "    \"\"\"\n",
    "    dir_path = path + \"/\" + target_model_name + \"/\" + dataset_name +  \"/\" + pretext_model_name + \"/loadedxFoldCrossVal_\" + str(pretext_model_xFold) \n",
    "    text_files = glob.glob(dir_path+'/checkpoint_*/txtlogs/eval.txt', recursive=True)  \n",
    "    if not text_files:\n",
    "        print(\"No textfile of \" + str(target_model_name) + \" for \" + str(pretext_model_name) + \"found.\")\n",
    "        return\n",
    "    \n",
    "    pretext_steps=[]      \n",
    "    best_epochs=[]\n",
    "    best_steps=[]\n",
    "    best_losses=[]\n",
    "    best_obj_losses=[]\n",
    "    best_reg_losses=[]\n",
    "    best_loss_accs=[]\n",
    "    best_accs = []\n",
    "    for file in text_files:\n",
    "        file_valid=0\n",
    "        pretext_steps.append(int(file.split(\"/checkpoint_\")[1].split(\"/\")[0]))\n",
    "        with open(file) as f:\n",
    "            content = f.readlines()    \n",
    "            for line in content:\n",
    "                if line.find(\"Best Loss Epoch\") != -1:\n",
    "                    best_epochs.append(int(line.split(\":\")[1].strip()))\n",
    "                    file_valid+=1\n",
    "                elif line.find(\"Best Loss Step\") != -1:\n",
    "                    best_steps.append(int(line.split(\":\")[1].strip()))\n",
    "                    file_valid+=1\n",
    "                elif line.find(\"Best Losses\") != -1:\n",
    "                    losses = line.split(\":\")[1].strip()[1:-1].split(\",\") \n",
    "                    best_losses.append(float(losses[0].strip()))\n",
    "                    best_obj_losses.append(float(losses[1].strip()))\n",
    "                    best_reg_losses.append(float(losses[2].strip()))\n",
    "                    file_valid+=1\n",
    "                elif line.find(\"Best Acc\") != -1:\n",
    "                    best_loss_accs.append(float(line.split(\":\")[1].strip()))\n",
    "                    accs = [float(line.split(\":\")[1].strip()) for line in content if \"Acc for epoch\" in line]\n",
    "                    best_accs.append(max(accs))\n",
    "                    file_valid+=1     \n",
    "                    \n",
    "        if file_valid is not 4:                                 \n",
    "            print(\"Textfile \" + str(pretext_steps[-1]) + \" could not be read\")\n",
    "            return None  \n",
    "        sort_index= np.argsort(pretext_steps)\n",
    "        \n",
    "    return np.array(pretext_steps)[sort_index], np.array(best_epochs)[sort_index], np.array(best_steps)[sort_index], np.array(best_losses)[sort_index], np.array(best_obj_losses)[sort_index], np.array(best_reg_losses)[sort_index], np.array(best_loss_accs)[sort_index], np.array(best_accs)[sort_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadTargetModelXFoldDataFromEvalTxtFile(path, target_model_name, dataset_name, pretext_model_name, pretext_model_xFolds):    \n",
    "    \"\"\"\n",
    "    Loads the stats of the target models train on every specified pretext training step for the given xFolds and dataset. \n",
    "    \n",
    "    :param path: (String) The path from which to get the states.\n",
    "    :param target_model_name: (String) The name of the target model for which to get the data every trained checkpoint.\n",
    "    :param dataset_name: (String) The names of the datasaset for which to get the data for every trained checkpoint.\n",
    "    :param pretext_model_name: (String) The name of the pretext model the target model was trained on.\n",
    "    :param pretext_model_xFold: (Integer) The xFold of the pretext model the target model was trained on.\n",
    "    \"\"\"\n",
    "    pretext_steps_per_xFold =[]      \n",
    "    best_epochs_per_xFold=[]\n",
    "    best_steps_per_xFold=[]\n",
    "    best_losses_per_xFold=[]\n",
    "    best_obj_losses_per_xFold=[]\n",
    "    best_reg_losses_per_xFold=[]\n",
    "    best_loss_accs_per_xFold=[]\n",
    "    best_accs_per_xFold=[]\n",
    "    \n",
    "    first_round = True\n",
    "    for xFold in pretext_model_xFolds:\n",
    "        pretext_steps, best_epochs, best_steps, best_losses, best_obj_losses, best_reg_losses, best_loss_accs, best_accs = loadTargetModelEvalTxtFile(path, target_model_name, dataset_name, pretext_model_name, xFold)\n",
    "                        \n",
    "        if not first_round:\n",
    "            if not (pretext_steps_per_xFold[-1] == pretext_steps).all():\n",
    "                print(\"Error xFolds lens do not match! \")\n",
    "                return None\n",
    "\n",
    "        pretext_steps_per_xFold.append(pretext_steps)\n",
    "        best_epochs_per_xFold.append(best_epochs)\n",
    "        best_steps_per_xFold.append(best_steps)\n",
    "        best_losses_per_xFold.append(best_losses)\n",
    "        best_obj_losses_per_xFold.append(best_obj_losses)\n",
    "        best_reg_losses_per_xFold.append(best_reg_losses)\n",
    "        best_loss_accs_per_xFold.append(best_loss_accs)\n",
    "        best_accs_per_xFold.append(best_accs) \n",
    "        first_round = False   \n",
    "        \n",
    "    return pretext_steps_per_xFold, best_epochs_per_xFold, best_steps_per_xFold, best_losses_per_xFold, best_obj_losses_per_xFold, best_reg_losses_per_xFold, best_loss_accs_per_xFold, best_accs_per_xFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadTargetModelTextEvalData(data_path, target_model_name, dataset_name, pretext_model_names, pretext_model_xFolds, steps_per_epoch):\n",
    "    \"\"\"\n",
    "    Loads the stats of the target models train on every specified pretext training step for the given xFolds and dataset. \n",
    "    \n",
    "    :param data_path: (String) The path from which to get the data.\n",
    "    :param target_model_name: (String) The name of the target model for which to get the best loss stats for every trained checkpoint.\n",
    "    :param dataset_name: (String) The names of the datasaset for which to get the stats for every trained checkpoint.\n",
    "    :param pretext_model_names: (Array of Strings) The names of the pretext model the target model was trained on.\n",
    "    :param pretext_model_xFolds: (Array of Integer) The xFolds of the pretext models the target model was trained on.\n",
    "    :param steps_per_epoch: (Integer) The number of steps in an epoch.\n",
    "    \"\"\" \n",
    "    epochs_per_xFold_per_model=[]\n",
    "    best_epochs_per_xFold_per_model=[] \n",
    "    best_steps_per_xFold_per_model=[]\n",
    "    best_losses_per_xFold_per_model=[]\n",
    "    best_obj_losses_per_xFold_per_model=[]\n",
    "    best_reg_losses_per_xFold_per_model=[]\n",
    "    best_loss_accs_per_xFold_per_model=[]\n",
    "    best_accs_per_xFold_per_model = []\n",
    "    \n",
    "    for pretext_model_name in pretext_model_names:\n",
    "        pretext_steps_per_xFold, best_epochs_per_xFold, best_steps_per_xFold, best_losses_per_xFold, best_obj_losses_per_xFold, best_reg_losses_per_xFold, best_loss_accs_per_xFold, best_accs_per_xFold = loadTargetModelXFoldDataFromEvalTxtFile(data_path, target_model_name, dataset_name, pretext_model_name, pretext_model_xFolds)\n",
    "        \n",
    "        epochs_per_xFold_per_model.append(np.divide(pretext_steps_per_xFold,steps_per_epoch))\n",
    "        best_epochs_per_xFold_per_model.append(np.array(best_epochs_per_xFold))\n",
    "        best_steps_per_xFold_per_model.append(np.array(best_steps_per_xFold))\n",
    "        best_losses_per_xFold_per_model.append(np.array(best_losses_per_xFold))\n",
    "        best_obj_losses_per_xFold_per_model.append(np.array(best_obj_losses_per_xFold))\n",
    "        best_reg_losses_per_xFold_per_model.append(np.array(best_reg_losses_per_xFold))\n",
    "        best_loss_accs_per_xFold_per_model.append(np.array(best_loss_accs_per_xFold))\n",
    "        best_accs_per_xFold_per_model.append(np.array(best_accs_per_xFold))\n",
    "        \n",
    "    return epochs_per_xFold_per_model, best_epochs_per_xFold_per_model, best_steps_per_xFold_per_model, best_losses_per_xFold_per_model, best_obj_losses_per_xFold_per_model, best_reg_losses_per_xFold_per_model, best_loss_accs_per_xFold_per_model, best_accs_per_xFold_per_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadTargetModelTensorboardEvalData(data_path, dataset_name, target_model_name, pretext_model_name, pretext_model_xFolds, steps_per_epoch, loss_enty_tag =\"loss\", obj_loss_enty_tag = \"obj_loss\", reg_loss_enty_tag = \"reg_loss\", acc_enty_tag = None):\n",
    "    \"\"\"\n",
    "    Loads the tensoboard data of the target model given by data_path, dataset_name, target_model_name, pretext_model_name and its xFolds.\n",
    "    \n",
    "    :param data_path: (String) The path  from which to get the data.\n",
    "    :param target_model_name: (String) The name of the target model for which to get the best loss stats for every trained checkpoint.\n",
    "    :param dataset_name: (String) The names of the datasaset for which to get the stats for every trained checkpoint.\n",
    "    :param pretext_model_names: (Array of Strings) The names of the pretext model the target model was trained on.\n",
    "    :param pretext_model_xFolds: (Array of Integers) The xFolds of the pretext models the target model was trained on.\n",
    "    :param steps_per_epoch: (Integer) The number of steps in an epoch.\n",
    "    :param loss_enty_tag: (String): The tag of the overall losses in the tensorboard data.\n",
    "    :param loss_enty_tag: (String): The tag of the objective losses in the tensorboard data.\n",
    "    :param loss_enty_tag: (String): The tag of the regularization losses in the tensorboard data.\n",
    "    :param loss_enty_tag: (String): The tag of the accuracies in the tensorboard data.\n",
    "    \"\"\"\n",
    "    trained_models_per_epoch = {}    \n",
    "    model_folder_path =  data_path + \"/\" +  target_model_name + \"/\" +  dataset_name + \"/\" + pretext_model_name                 \n",
    "    dir_prefix = model_folder_path + \"/loadedxFoldCrossVal_\"\n",
    "    eval_dirs = glob.glob(model_folder_path + \"/loadedxFoldCrossVal_0/checkpoint_*/logs/eval\", recursive=True) \n",
    "    eval_dirs.reverse()\n",
    "    eval_dirs = [eval_dirs[i].split(\"/checkpoint\")[1] for i in range(len(eval_dirs))]    \n",
    "                          \n",
    "    for dir_suffix in eval_dirs:     \n",
    "        pretext_model_epoch = int(dir_suffix.split(\"_\")[1].split(\"/\")[0])/steps_per_epoch                   \n",
    "        steps_per_xFold, losses_per_xFold, _, _, _ = loadModelTensorboardData(dir_prefix, \"/checkpoint\"+dir_suffix, pretext_model_xFolds, loss_enty_tag = loss_enty_tag, obj_loss_enty_tag=obj_loss_enty_tag, reg_loss_enty_tag=reg_loss_enty_tag, acc_enty_tag = acc_enty_tag)\n",
    "   \n",
    "        if pretext_model_epoch not in trained_models_per_epoch.keys():\n",
    "            trained_models_per_epoch[pretext_model_epoch]={}\n",
    "        if \"xFold_epochs\" not in trained_models_per_epoch[pretext_model_epoch].keys():\n",
    "            trained_models_per_epoch[pretext_model_epoch][\"xFold_epochs\"]=[]\n",
    "        if \"xFold_losses\" not in trained_models_per_epoch[pretext_model_epoch].keys():\n",
    "            trained_models_per_epoch[pretext_model_epoch][\"xFold_losses\"]=[]\n",
    "                                       \n",
    "        trained_models_per_epoch[pretext_model_epoch][\"xFold_epochs\"].append(np.divide(steps_per_xFold,steps_per_epoch)[0]) \n",
    "        trained_models_per_epoch[pretext_model_epoch][\"xFold_losses\"].append(losses_per_xFold[0])  \n",
    "\n",
    "    return trained_models_per_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printMeanMinMax(prefix, name, values):\n",
    "    \"\"\"\n",
    "    Helper function for debugging.\n",
    "    \"\"\"\n",
    "    max_value=np.max(values)\n",
    "    min_value=np.min(values)\n",
    "    mean_value=np.mean(values)\n",
    "    print(prefix +\n",
    "                      \" | Max \"+ str(name) + \": \" + str(np.around(max_value, 2)) + \n",
    "                      \" | Min \"+ str(name) + \": \" + str(np.around(min_value, 2)) + \n",
    "                      \" | Mean \"+ str(name) + \": \" + str(np.around(mean_value, 2))  + \n",
    "                      \" | + \"+ str(name) + \": \" + str(np.around((max_value-mean_value), 2)) + \n",
    "                      \" | - \"+ str(name) + \": \" + str(np.around((min_value-mean_value), 2)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
